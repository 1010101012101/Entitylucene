Automate daily saving of webarchive? <body> Is it possible to automate the saving of a webpage (using the code.webarchive/code format) using either Automator (in a background process) or using Terminal?  <answer116285> Yes is the simple answer with either.   I am on my iPad at the mo. But you can use unix command curl to download the webpage to and pipe it to the unix command textutil which can output it to a webarchive file.  If I get a chance I will post an example.  hr  Here is a small example (quick ) of what I was thinking. Written in Applescript running do shell script commands.  precode  property agent : "Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.1.3) Gecko/20090824 Firefox/3.5.3"  property outPutFormat : "rtf" property saveDIR : "/Users/USERNAME/Desktop/" property fileName : "test2"  set theData to do shell script "curl " &amp; " -A" &amp; space &amp; quoted form of agent &amp; space &amp; "http://weather.yahoo.com/france/auvergne/france-29332634/" as string   do shell script "echo " &amp; quoted form of theData &amp; "|textutil -format html -convert" &amp; space &amp; outPutFormat &amp; space &amp; "-stdin -output " &amp; space &amp; saveDIR &amp; fileName &amp; "." &amp; outPutFormat /code/pre  Although this works. I am not very happy with the results. This is due to curl and textutil only processing the html code but not resources.  So am working on something else that will save a webArchive in a much better way. 90% there but will take a little longer for me to write  <answer120548> h3Downloading &amp; saving as webarchive/h3  A command line tool named webarchiver will download URLs and save them to code.webarchive/code format. You can install this tool via MacPorts (alas, not homebrew!) or compile it with XCode. I am a XCode dummy, but succeeded with instructions found here.  How to operate:  precodewebarchiver 0.5 Usage: webarchiver -url URL -output FILE  Example: webarchiver -url http://www.google.com -output google.webarchive -url    http:// or path to local file -output File to write webarchive to /code/pre  h3Nice file names/h3  This lenghty one-liner for terminal allows you to configure the desired URL and will download a YYYY-MM-DD-prefixed webarchive file:  codeURL="www.nytimes.com"; ./webarchiver -url "http://$URL" -output "/Users/&lt;your username&gt;/Desktop/$(date +"%Y-%m-%d-$URL.webarchive")"/code  This will save a webarchive to your Desktop:  code2014-02-10-www.nytimes.com.webarchive/code  If you are not sure what code&lt;your username&gt;/code is, enter codewhoami/code in Terminal.app (and press enter, of course).  h3Cron/h3  I would rather use codelaunchd/code, as "the use of cron on OS X is discouraged". There is a nice launchd editor named Lingon. Have fun!  <comment136023> I really would appreciate your following up on this. Would it also be possible for you to show how this can be done in the context of a cron(job)? Thanks in advance! <comment136046> what is the webpage. Also in my haste to help you I forgot doing it this way does work. But only for the front facing page. But some resource file etc. will not be included so the page will look wonky. Still looking at way to resolve that. But can you supply more deatil on why and what you want from the page. <comment136134> Right. I was looking to capture pages of news sites like ft.com, nytimes.com, ..., so I am able to see the headline trends throughout the months and years. My memory isn't great so this would be a great aid. <comment136145> So rather than a webarchive would a text file be better. Maybe formatted. I think it would work better <comment136383> Yeah, wouldn't mind having a formatted text file. Would it be possible for you to post an example? <comment136561> I have added a small example. But I am working on something else that may work better.. <comment136655> Thank you for the initial AppleScript code. A .webarchive would be absolutely fantastic. <comment136818> @JFW, Just to keep you up to date. I am nearly there with this. Just a couple of bugs (that I know of) to iron out and I can let you test it. <comment136858> Fantastic. Really, no worry, in regards to the time. I'm just doing it manually at the moment but it would be fantastic if I could expand it so that I could have 8 or 9 URLs within the script that I can run every single day (or click, if it's an Automator application) that can be automatically saved to the .webarchive format. <comment164983> Brilliant find! Thank you so much! I'm planning on automating this with Automator (running each website archive within the 'Run Shell Script' option. Thing is, is there any way to select where the output will be? (Right now the default area seems to be ~/user instead of ~/user/desktop) Thanks. <comment165205> You are welcome! Please see the updated answer. <comment175527> Thanks again for finding it. I've marked your answer as the one that has solved the question (after the question was first asked 275 days ago). I've been using webarchiver since and it's been incredible. I've not had the guts to automate this, so what I did was just put the line of code into Automator, and output that as a clickable file on the desktop. It's been pretty fantastic so far. Unfortunately the file sizes of the output are pretty big (~3MB, as opposed to the usual 0.5MB), but the fact that it is self-contained means that it's a lot more portable. <comment371727> Just a heads up: webarchiver has been made available through homebrew as well (have a look: `brew info webarchiver`)!