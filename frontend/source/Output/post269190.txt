How to tell kernel_task not to compress a large size of shared memory created by Python multiprocessing? <body> I am using Python to do big data analysis in parallel. The data is about 9G (the Mac has 16G RAM in total) and all read in memory and is shared (and read-only):  precodeimport multiprocessing, ctypes from multiprocessing.pool import Pool SharedArray = multiprocessing.Array(ctypes.c_double, l*n) # l*n is huge! inputs = numpy.ctypeslib.as_array(SharedArray.get_obj()).reshape((l, n))  pool = Pool(4) evol = ClassOfAPopulationOfParameters(populationSize=100) population = pool.map(FunctionUsing_inputs, evol.population, chunksize=1) /code/pre  When the code is running, 4 subprocesses are created. Although they use a chunk of shared memory, each of them shows a memory consumption of ~9G in Activity Monitor. However, total memory used is &lt;10G and there is 6G free space. So, the memory of the inputs is indeed shared by the 4 subprocesses but Activity failed to show it properly.  What is worse, this problem frequently triggers kernel_task to compress the memory of my program. It uses 50%~200% of CPU and limits each of my processes to use only &lt;25% CPU! However, if I only load slightly less data, kernel_task would not be trigger and my processes all happily use close to 100% CPU.  My question is, how can I avoid the problem caused by kernel_task's failure to understand that the data are shared? How can I avoid its activation? I don't need it to compress the memory because there are still 6G free RAM. Please do not suggest not loading all the data and do not suggest "sudo nvram boot-args="vm_compressor=1"" because that somehow caused the OS to crash.  Thanks! 