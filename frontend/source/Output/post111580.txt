How to comprehensively benchmark SSDs on OS X? <body> I want to benchmark my SSD under different circumstances. Connected through USB3, Thunderbolt and internally (S-ATA).   Unfortunately I can only find benchmarks which concentrate on measuring read and write speeds. I am also (and mostly) interested in the latency.   I tried Xbench and Blackmagic Disk Speed Test.   Is there a tool which can benchmark storage devices including their latency?  <answer130916> Those utilities you list likely do test drive latency, but only for HDD.  You really can't test the latency of an SSD, because there are no spinning parts to bring the required disk sector under the read-write head, and there is no read-write head for that matter. HDD metrics like rotational latency and seek times don't apply:     For SSDs, most of the attributes related to the movement of mechanical components are not applicable, but the device is actually affected by some other electrically based element that still causes a measurable delay when isolating and measuring that attribute   There is a latency metric for SSD, but you still can't test it, or it won't tell you anything of value if you could, unless you have a certain kind of ementerprise design/em:     Today low latency can only effectively be addressed by one particular type of storage architecture and that’s an enterprise SSD design. Latency in a technical environment is synonymous with delay. More succinctly latency in terms of a SSD is how long it will take for a request to complete its round trip cycle from the time the request enters the device to the time that it leaves the device with the “payload” in tow.    The latency will be exactly the same for all the examples you mention because it is a metric internal to the SSD (see above "from the time the request ementers/em the device to the time that it emleaves/em the device with the “payload” in tow.").   If you want to know what that value is for your SSD, check with the manufacturer.   <answer130938> A great tool for doing this type of benchmarking is emfio/em  For Mac OS X it is available through homebrew. You install it with the simple command:  brew install fio  This command requires that you have homebrew installed in advance.  If you haven't, go to this web site and grab it:  http://brew.sh/  <answer284243> For the OP, look up emfio/em, I think someone mentioned it somewhere on this. It's the best way to explore the limits of the SSD on your Mac. Just be sure to use the 'direct=0' argument, or you may bork your filesystem.   That said...  Some of the answers here are just absurd! I know the replies are years old at this point, but you absolutely can, and SHOULD, measure IO Latency of an SSD.   tl;dr: SSDs have IO latency like any other storage system, you can measure it.   There's a distinct difference between 'Seek Time', a mechanically-bound metric associated with spinning disk, and 'IO Service Time' which is an aggregate of the latency associated with each step of the I/O process. In environments with spinning disks (including home PCs and Macs), Seek Time is one such step, and can be quite significant in poorly designed/cached/maintained systems (which is probably the vast majority of business systems, and nearly every home system not run by someone skilled in the realm of storage performance tuning).  Generally, folks are referring to this emIO Service Time/em when they say 'latency', and emnot/em 'Seek Time', as that metric has become increasingly irrelevant as:  ol liFlash-based storage devices become the norm/li liSpinning disks inside and outside of the enterprise IT shop bump up against mechanical limitations which simply cannot be exceeded due to physics/li liSystems needing real performance rely on software optimizations (caching implementations, IO coalescing, maintenance of stripe sets and other tricks to ease the grip Newtonian physics has on the storage system's disks) or expectation management ("If you want faster, don't be cheap, buy flash, otherwise hush.") as most of the ultra-high-performance storage consumers have moved to Flash long ago, taking the pressure off of spinning disk manufacturers to push for that last quarter of a millisecond of seek time performance/li /ol  When we are evaluating a storage system, we are talking in terms of microseconds most of the time. These days, if a storage system cannot provide consistently sub-5ms performance under emany/em load (for general workloads), it can't hang. For the high performance consumers, we expect consistent sub-1ms response times for the critical workload profiles.   The fastest spinning disks you can get have emseek times/em of over 2ms alone, and that's the best case scenario. Add in the fact that if you're in a RAID array scenario, every disk needs to seek to the desired stripe and they may be all over the place as far as head position, so it can get pretty ugly. (In a pure hardware RAID scenario without caching strategies to lessen this, see above.) Add in to that the time it actually takes to move the desired data, compute parity if necessary, update stripes where applicable... You can see, the benefits of SSDs become obvious immediately, in terms that aren't very hard to articulate, once you know what's up. Hope this rant helps googlers out there in their quest for the knowledges.    <comment153674> You reply does not make any sense. Ofcourse you can test the latency of your SSD - it doesn't depend on a specific "enterprise design". Nor is it a specific value that you can ask your manufacturer for - the latency will vary with the type of operation and whatever your SSD is doing when you perform the request. The SSD might have a backlog or be doing garbage collection in the background, you could have command queuing enabled, etc. - all things that will affect latency. <comment153723> @jksoegaard : it makes perfect sense. You can test the latency, but [doing so will tell you nothing](https://www.eiseverywhere.com/file_uploads/c8d558cf6dc9b20a5b967f4949e85a15_Monday_1110_LeviNorman_Latency.pdf) Latency of an SSD is a metric of how long it takes from when the request enters the SSD to the time when it leaves the SSD. So if you test it, in all three examples, you'll get the same value, thus telling you nothing about which way is best for you. The latency will not change! An SSD has a certain latency that it will have for every operation, it only differs in different SSD's. <comment153724> @jksoegaard - furthermore, in regards to SSD latency metrics, we're talking about milliseconds. Beyond the fact that it will always be the same for a single SSD, it will have a value of thousandths of seconds. In a personal home set up, with less than 10 users of, say, a server set up, this is negigible in all situations. Only when compounding that in an enterprise set up, with thousands or 10s of thousands of *simultaneous* requests will it make any difference whatsoever. <comment153913> I don't see why that should be the case at all. Your idea that latency is only measured when the request "enter the SSD" and "leaves the SSD" is simply not correct. You can ofcourse measure latency through the whole stack/chain, including interface such as USB. Secondly, the notion that each SSD will have a constant latency does not ring true. The latency can vary depending on the circumstances that I mentioned earlier. <comment153915> Your second comment saying that latency is not important - I don't see that as an answer at all. You cannot know what Stefans usage scenario is, as he hasn't detailed it - thus you cannot know in advance if latency is going to be important or not. Similarly there's nothing wrong with wanting to measure something just for the sake of measuring it. I'm well aware what kind of range SSD latencies are typically in - and I do know that they can vary a lot (relatively) between different makes and models, and in various situations. <comment153917> You really do not need 10s of thousands of simultaneous requests for SSD latency or other disk latency to matter. For example I'm using SSDs together with Ceph to provide access to storage over a network. The total latency of SSD, bus, software, etc. really matters quite a bit there - much more so than the total bandwidth available in my case. You might say that's an "enterprise scenario", but that's a pretty vague term. Today's grandmother's home computer beats most enterprise setups 15 years ago.