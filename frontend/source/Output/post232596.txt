Shell command or series of commands similar to the Automator action "Get Link URLs from Webpages" <body> I have a long list of URLs. Contained within each of these webpages, there are links I need to extract. The Automator action Get Link URLs from Webpages is a very useful action for this task. Unfortunately Automator itself does not handle heavy workloads very well and very often either crashes or hangs indefinitely. How might I go about this using Bash via the Mac OS X Terminal app?  Edit - this is the current script as it stands right now.     precode#!/bin/bash  echo "Enter up to 3 words" read -p "" v1 v2 v3   web="$HOME/web.txt" tmp="$HOME/tmp.txt" err="$HOME/err.txt" fin="$HOME/fin.txt" arc="$HOME/arc.txt"  n="$(awk 'END {print NR}' "$web")" echo "Processing $n URLs..."  grep 'http' "$web" | \ while read -r url; do     lynx -nonumbers -hiddenlinks=merge -dump -listonly "$url" 2&gt;&gt;"$err" | awk '!a[$0]++' &gt;&gt; "$tmp"     sleep 1     n=$((n-1))     [[ $n -gt 0 ]] &amp;&amp;  echo "$n URLs left to process..." || echo "Processing Completed!" done  grep -e "$v1" -e "$v2" -e "$v3" "$tmp" | sort -u | cat &gt; "$fin"  cat "$fin" &gt;&gt; "$arc"  for r in "Results This Session"; do echo "$(cat "$fin" | wc -l)" "$r"; done for a in "URL's Archived"; do echo "$(cat "$arc" | wc -l)" "$a"; done /code/pre  I added coderead -p/code to the beginning of the script. Are there limitations on the number of variables that can be used like this? I've successfully used up to 9 in testing. And is there a more practical way to write this? I tried coderead -p "" {v1..v9}/code which didn't work. I added a couple codefor/code loops at the end to indicate how much data was processed.   h3Current issues/h3  ul lisometimes I get an error   precodesort: string comparison failed: Illegal byte sequence sort: Set LC_ALL='C' to work around the problem. /code/pre  however when add codeLS_ALL=C/code to the script it doesn't seem correct this./li /ul  <answer232616> Here's a script to get you started:  precode#!/bin/bash  urls="/path/to/Input_URLs_List.txt" output="/path/to/Output_Link_URLs.txt"  n="$(awk 'END {print NR}' "$urls")" echo "Processing $n URLs..."  cat "$urls" | \ while read url; do     lynx -dump -listonly "$url" &gt;&gt; "$output"     sleep 5     n=$((n-1))     echo "$n URLs left to process..." done /code/pre  This will dump all of the links into a file that you can further process based on what you're looking for.  Additional emcode/em could be added to filter and process the output however without knowing what it is you need you'll have to work on it or ask some additional questions.  hr  To clean up the emoutput/em, use the following as an example:  Using "https://www.google.com" as one of the URLs the emoutput/em would look like:  precode$ lynx -dump -listonly "https://www.google.com"   References      1. https://www.google.com/imghp?hl=en&amp;tab=wi     2. https://maps.google.com/maps?hl=en&amp;tab=wl     3. https://play.google.com/?hl=en&amp;tab=w8     4. https://www.youtube.com/?tab=w1 /code/pre  I've truncated the output, there's actually 19 Link URLs.  To have the emoutput/em just be a list of URLs, no numbers or whitespace, etc., use codeawk/code either in conjunction with codelynx/code or afterwards.  precode$ lynx -dump -listonly "https://www.google.com" | awk '/:/{print $2}' https://www.google.com/imghp?hl=en&amp;tab=wi https://maps.google.com/maps?hl=en&amp;tab=wl https://play.google.com/?hl=en&amp;tab=w8 https://www.youtube.com/?tab=w1 /code/pre  So if you want the emoutput/em file to be just the Link URLs, change the codelynx/code command line to:  precodelynx -dump -listonly "$url" | awk '/:/{print $2}' &gt;&gt; "$output" /code/pre  You can always process the contents of the emoutput/em file later in the emscript/em or afterwards to get it down to the really wanted Link URLs and use a different search parameter in codeawk/code, e.g., I used ":" to both eliminate the blank lines in the codelynx/code emoutput/em and to show an example of how it can be filtered.  In this example only Link URLs get redirected into the emoutput/em file because only lines containing a code:/code get output by codeawk/code, as all URLs should have a colon in them. The code{print $2}/code, simplified in this explanation, removes everything to the left of the actual Link URL.   hr  Here's an updated emscript/em which sorts and removes duplicate Link URLs:  precode#!/bin/bash  urls="/path/to/Input_URLs_List.txt" output="/path/to/Output_Link_URLs.txt"  n="$(awk 'END {print NR}' "$urls")" echo "Processing $n URLs..."  cat "$urls" | \ while read url; do     lynx -dump -listonly "$url" | awk '/:/{print $2}' | sort | uniq &gt;&gt; "$output"     sleep 5     n=$((n-1))     [[ $n -gt 0 ]] &amp;&amp;  echo "$n URLs left to process..." || echo "Processing Completed!" done /code/pre  hr  strongUpdate to capture codestderr/code emoutput/em from codelynx/code to a file:/strong  To capture codestderr/code emoutput/em from codelynx/code to a file, redirect codestderr/code to a disk file, e.g., code2&gt;&gt;"$file"/code added after code"$url"/code, e.g.:  precodelynx -dump -listonly "$url" 2&gt;&gt;"$file" &gt;&gt; "$output" /code/pre  Add codeerrlog="/path/to/Lynx_Errors.txt"/code under codeoutput="/path/to/Output_Link_URLs.txt"/code and then change the codelynx/code command line to, e.g.:  precodelynx -dump -listonly "$url" 2&gt;&gt;"$errlog" &gt;&gt; "$output" /code/pre  Or:  precodelynx -dump -listonly "$url" 2&gt;&gt;"$errlog" | awk '/:/{print $2}' | sort | uniq &gt;&gt; "$output" /code/pre  strongExample:/strong  precode#!/bin/bash  urls="/path/to/Input_URLs_List.txt" output="/path/to/Output_Link_URLs.txt" errlog="/path/to/Lynx_Errors.txt"  n="$(awk 'END {print NR}' "$urls")" echo "Processing $n URLs..."  cat "$urls" | \ while read url; do     lynx -dump -listonly "$url" 2&gt;&gt;"$errlog" | awk '/:/{print $2}' | sort | uniq &gt;&gt; "$output"     sleep 5     n=$((n-1))     [[ $n -gt 0 ]] &amp;&amp;  echo "$n URLs left to process..." || echo "Processing Completed!" done /code/pre  <comment284236> I've actually already got lynx installed via Homebrew. <comment284245> Okay then. IMO writing a bash script using lynx to get the links from the list of URL's is the way to go. With the target URLs in a text file, one per line, the file can be read in a line at a time and processed in a loop with a timer so as not to hammer the Server to fast if URL's are pointing to the same domain and or just to pace things appropriately. All the output gets put into another file to be filtered as needed to get to as list of wanted URL's. Do you need help with the script? <comment284246> Yes that would be great if you could.  I'm starting to learn bash but I'm very new to it. I've got the URLs in a plain text file one per line. Just not sure where to go from there. <comment284301> I know we're not supposed to use comments for saying thanks but to hell with the rules.. Thank you very much! You have helped me tremendously. <comment284321> @user556068, I've added an example to filter the `lynx` _output_, so the _outfile_ will only contain Link URLs, no numbers or whitespace, etc. <comment284325> So much information. I can tell I'm about to learn a lot of new things. Question for you - How would I go about creating an error log to keep track of any URLs that cause "lynx: Can't access startfile" error messages? <comment284328> @user556068, See updated answer to capture `stderr` _output_ from `lynx` to a file. <comment284725> Instead of `lynx.... | awk.... | sort | uniq` try `lynx -nonumbers -hiddenlinks=merge -dump -listonly "$url" | awk '!a[$0]++'` <comment284726> @fd0, Sounds good. I'm out for a few hours and when I get back I'll test it out, as I always test code before I post it... So it's nothing personal as I can say I've learned more than a thing or two from your other answers. :) <comment284731> @fd0 I appreciate your suggestion. I'm testing a modified version at this very moment. Quick question for you both - you guys suggest using `sort | uniq` but would it not be better to use `sort -u` instead? <comment284736> @user556068 The `-listonly` option will list the URLs in the order in which they are found. `awk '!x[$0]++'` will remove duplicates after a URL is listed. If preserving the order isn't necessary then you could use `sort -u`. Such as:  `lynx -nonumbers -hiddenlinks=merge -dump -listonly "$url" | sort -u` <comment284739> @fd0 I see so `awk '!x[$0]++'` is taking care of that already while still preserving the order in which they are found. This is more desirable than `sort -u` I think. Thank you. <comment285078> @user556068 If you edit your original post adding the script as it now stands. I'll make some suggestions. <comment285254> @fd0, I have updated the original question with the script as it is at this point in time. <comment285370> @user3439894, I have posted a working ( I think ) version of the script. I'd love to get some feedback from you when you get a chance. I hope I haven't mangled your original code too much. <comment285837> I think I was wrong after all about the `sort -u` command. When looking at the different outputs I found that the file I dumped everything to first before using `sort -u` has entries that list the main site and then recursively lists the other sites which link to the content in question. So yeah.. you guys had it right the first time.. which I'm sure you already knew. ;)