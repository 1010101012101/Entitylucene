Is the time required for incremental Time Machine backups greatly affected by the number of files? <body> Is the time required for incremental Time Machine backups greatly affected by the number of files being backed up? I have a large number of files on my Mac that very rarely change and aren't "mission critical". If they greatly affect backup time I'll exclude them from Time Machine but if they don't I'll just backup everything. Backup size is a non-issue for me.  If it matters, I have Time Machine backing up to a remote box that is usually on the local network.  <answer255374> Incremental backups only backup what has changed. The number of unchanged files is irrelevant during an incremental backup. If you suddenly change all of the "large number of files", the next incremental backup will take significantly more time.  <comment318042> I need a bit more evidence. One could imagine a worse case but reasonable implementation where Time Machine hashes every file that *may* need to be backed up. In that case the number (and size) of files certainly could slow down back ups even when nothing changes. <comment318043> I have 243 million files on my hard drive, according to Disk Utility. Looking at Console for the Time Machine process `backupd`, it tells me that at 6:11:23 PM last evening it started an incremental backup process. At 6:11:28 it had determined 529 files needed to be backed up. Granted, there are some excluded files so the 243 million number is high, but even if it's actually only half that, the process you describe isn't possible in 5 seconds for 121.5 million files. Looking back through the logs, the 4-5 second timeframe is pretty consistent. <comment318054> That methodology seems fair but situation specific so I should test it for myself. How do I determine the start and stop time for a backupd process like you did? Also, if you know, it would be good to add to the answer how Time Machine identifies modified files so fast. Is it mtime?