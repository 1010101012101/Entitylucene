Slow performance accessing Ubuntu Samba folders <body> I have a computer at home with an strongUbuntu Server 14.04/strong. This server has several hard disks that are shared using strongSamba/strong.  I have an strongiMac/strong with strongOS X El Capitan 10.11.5/strong and I use one of the shared disks to store some information. I'm accessing this disk a lot of times to write and read information.  The performance of the Finder is really slow compared with my Ubuntu laptop and my girlfriend window's laptop...  The configuration for the disk on Samba is this:  precode[MYDATA]     comment = my coment...     path = /path/to/disk     browsable = yes     guest ok = yes     read only = no     create mask = 0755 /code/pre  I would like to know if the configuration of the disk should be different in any way, or maybe if I need to add some configuration in my Mac.   I also noticed that sometimes when I try to rename a folder inside this Samba share my Mac usually ask for my root password to modify the folder's name.  <answer239559> You should avoid using SMB on OS X (IMHO) as it has been buggy since the days of Snow Leopard (10.6), for me at least.  A quick Google search limited to this year alone brings up a myriad of issues regarding OS X and slow SMB implementations.   There are a few  things you can try:  ul liUse the SMB2 protocol by mounting your share  codecifs://&lt;whatever your share name is&gt;/code/li liEnable AFP (Netatalk) on your Ubuntu server and connect to the mount points using codeafp://&lt;whatever your share name is&gt;/code /li liSetup NFS on Ubuntu  and mount them on OS X /li /ul  Personally, if I have found CIFS good, AFP better, and NFS (even though it's much older) to be the best in terms of performance.    <answer240286>    Enable AFP (Netatalk) on your Ubuntu server and connect to the mount points using afp://   And get a lot of  ._*,.DS_Store,.TemporaryItems,.apdisk,.Appledouble ones in each folder)))  <comment294005> Thanks ! I solved my problem with `cifs://MYSERVERNAME`. And the finder works pretty much better with this <comment294006> Glad I could help....I was in the same spot you were in...for ***weeks*** ...thinking it was me messing up my configuration somehow.