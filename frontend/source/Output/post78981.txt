Can I download all pictures on a page? <body> Is there a way I can run a script with a certain web page as the argument that will download all the images from there?  <answer78995> Here is a hacky solution (but it works). Hope someone can find a better one.  ol liIn Terminal, use codewget --page-requisites http://example.com//code. This will download the webpage at example.com and all of the resources linked from it (such as images, stylesheets, and scripts). More info on --page-requisites. Note: You can add many URLs separated by spaces to download lots of them at once. If many are from the same server, you should use something like codewget --wait=2/code to avoid slurping down files too fast./li liOpen the folder you downloaded those files to and use Spotlight to separate the images from the other files. I'm going to assume you have Mountain Lion. Type "Image" into the search field and select Kinds  Image./li /ol  <answer79148> precodewget -nd -r -l1 -p -np -A jpg,jpeg,png,svg,gif -e robots=off http://www.apple.com/itunes/ /code/pre  ul li-nd (no directories) downloads all files to the current directory/li li-r -l1 (recursive level 1) downloads linked pages and resources on the first page/li li-p (page requisites) also includes resources on linked pages/li li-np (no parent) doesn't follow links to parent directories/li li-A (accept) only downloads or keeps files with the specified extensions/li li-e robots=off ignores robots.txt and doesn't download a robots.txt to the current directory/li /ul  If the images are on a different host or subdomain, you have to add -H to span hosts:  precodewget -nd -H -p -A jpg,jpeg,png,gif -e robots=off http://example.tumblr.com/page/{1..2} /code/pre  You can also use curl:  codecd ~/Desktop/; IFS=$'\n'; for u in $(curl -Ls http://example.tumblr.com/page/{1..2} | sed -En 's/.*src="([^"]+\.(jpe?g|png))".*/\1/p' | sort -u); do curl -s "$u" -O; done/code  -L follows location headers (redirects). -O outputs files to the current directory with the same names.  <answer79164> You can use an Automator workflow to download images embedded in a web page, or images linked from a web page. A good starting point for a workflow is:  ol liGet Current Webpage from Safari/li liGet Image URLs from Webpage/li liDownload URLs/li /ol  img src="https://i.stack.imgur.com/gbNns.png" alt="Downloading images from web pages with Automator on Mac OS X 10.8"  You can change the workflow to use a list of web pages to fetch from.  Automator is included with Mac OS X in the codeApplications &gt; Utilities/code folder.  <answer81312> If you know the pattern in the url, you could use the *ix solution with Curl: Use curl to download images from website using wildcard?  <answer143554> Check out the Automator Space on MyAppleSpace http://www.myapplespace.com/pages/view/14664/automator-script-library  <comment91296> Have you looked at Automator? There is an Get Image URL from webpage plug in already written. Be careful with overly broad questions (as well as flooding the site with too many questions and no answers - the site works best when you give back as well as just ask - even if the asks are all good). <comment91411> It only downloaded the HTML code of it, not any of the images. It appears to be the same as if I had done "Save As..." in my web browser. <comment91412> Either the site is using JavaScript to load in the content, or they block `wget` by user agent.  In the second case, you can try using `wget -U "enter your web browser's user-agent here"` to pretend you are an actual web browser. <comment91416> @JShoe I just tested the -U flag with Safari's user agent, and it works. <comment91417> I was using Chrome, and trying to download from imgur. Also, what is a user-agent? <comment91418> @JShoe The User-agent is what a browser or client like wget uses to identify itself to a server. [This website](http://httpbin.org/user-agent) will show you the user-agent string that your browser sent to its server. That is what I used with wget -U. <comment91419> Okay, so I successfully ran the updated command, but it still only downloaded the HTML code. <comment91444> let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/7134/discussion-between-kevin-chen-and-jshoe) <comment93736> Copy all image urls? `Lynx -dump` could work in downloading but a lot of parsing -- no easier method? -1 unless the second point clarified -- it can be a lot of work... <comment93737> You could use `Lynx -dump` and parse all image-urls there or perhaps some scraper, trying to find better solution although this is nice -- could you explain how you got GNU coreutils in OS X? <comment93771> @hhh the second section, get image URLs, is performed by the Automator action displayed in the screen shot. There is no significant work for the user. <comment93840> +1 hey this is cool, why cannot I change my downvote to upvote? I did not know this is so easy! Thanks for sharing :) <comment94130> @hhh I compiled it from source, but you can also use Homebrew or MacPorts. (I think it used to come with OS X?) <comment169092> This isn't unnecessary as Automator already has this. <comment169099> ...and link-only answers are discouraged and the accepted answer already shows how to do it in Automator.