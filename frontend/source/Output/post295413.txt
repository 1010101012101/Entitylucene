compress Long size file into multiple limited size xz file <body> I have files of 28GB size.  I want to compress using code.xz/code LZMA, with .xz chuncks of 1G (1024MB) size files...  precodeMyFile.vdi -&gt; MyFile00.xz, MyFile01.xz, MyFile02.xz... /code/pre  strongHow do it in macOS/strong?  strongEDIT/strong  precodesh-3.2# xz -ek9vfc Centos\ 7\ 1511\ 64bits_20170629\(with_dukto\).vdi xz: Compressed data cannot be written to a terminal xz: Try `xz --help' for more information. sh-3.2#  /code/pre  strongSECOND EDIT/strong  USING  precode# xz -ek9vfc Centos\ 7\ 1511\ 64bits_20170629\(with_dukto\).vdi  # xz -e9vfc &lt; Centos\ 7\ 1511\ 64bits_20170629\(with_dukto\).vdi | split -b 1024m - Centos\ 7\ 1511\ 64bits_20170629\(with_dukto\).vdi.xz /code/pre  Here the output.  precode# ls -al Centos\ 7\ 1511\ 64bits_20170629\(with_dukto\).vdi* -rw-------@ 1 _unknown  _unknown  17470324736 Jun 29 23:47 Centos 7 1511 64bits_20170629(with_dukto).vdi -rw-r--r--  1 _unknown  _unknown   3898588580 Aug 22 22:19 Centos 7 1511 64bits_20170629(with_dukto).vdi.xz -rw-r--r--  1 _unknown  _unknown   1073741824 Aug 23 12:15 Centos 7 1511 64bits_20170629(with_dukto).vdi.xzaa -rw-r--r--  1 _unknown  _unknown   1073741824 Aug 23 13:44 Centos 7 1511 64bits_20170629(with_dukto).vdi.xzab -rw-r--r--  1 _unknown  _unknown   1073741824 Aug 23 18:17 Centos 7 1511 64bits_20170629(with_dukto).vdi.xzac -rw-r--r--  1 _unknown  _unknown    677363108 Aug 23 21:44 Centos 7 1511 64bits_20170629(with_dukto).vdi.xzad sh-3.2#  /code/pre  emHere the help/em  precode# xz -h Usage: xz [OPTION]... [FILE]... Compress or decompress FILEs in the .xz format.    -z, --compress      force compression   -d, --decompress    force decompression   -t, --test          test compressed file integrity   -l, --list          list information about .xz files   -k, --keep          keep (don't delete) input files   -f, --force         force overwrite of output file and (de)compress links   -c, --stdout        write to standard output and don't delete input files   -0 ... -9           compression preset; default is 6; take compressor *and*                       decompressor memory usage into account before using 7-9!   -e, --extreme       try to improve compression ratio by using more CPU time;                       does not affect decompressor memory requirements   -T, --threads=NUM   use at most NUM threads; the default is 1; set to 0                       to use as many threads as there are processor cores   -q, --quiet         suppress warnings; specify twice to suppress errors too   -v, --verbose       be verbose; specify twice for even more verbose   -h, --help          display this short help and exit   -H, --long-help     display the long help (lists also the advanced options)   -V, --version       display the version number and exit  With no FILE, or when FILE is -, read standard input.  Report bugs to &lt;lasse.collin@tukaani.org&gt; (in English or Finnish). XZ Utils home page: &lt;http://tukaani.org/xz/&gt; sh-3.2#  /code/pre  <answer295420> If a text file, you could use the terminal command split to split your initial file into smaller files and then compress them all in a batch file. The man page on split will tell you how to determine the size of smaller files. The split command will make as many output files as needed based on how large you told it to makes each file. Using batch compress makes this pretty time efficient.  <answer295607> Use codesplit/code.   Example:  codexz -e9vfc &lt; yourFile.vdi | split -b 1024m - yourFile.vdi.xz./code  This will split yourFile.vdi into compressed chunks of 1GB, suffixed by aa-zz.   To reassemble, do codecat yourFile.vdi.xz.* &gt; yourFile.xz/code  7-zip is MORE capable, but less ubiquitous. You'll want to checksum your backups and verify it every once in a while. Have a look at a GUI package of p7zip. It will let you select compression levels and volume sizes of 1GB... If you're doing this in an unattended batchjob, p7zip is dirtier. I can find one of my snippets if you need it.  A nice thing about xz though, is that you can assign n cores to a job. I believe it's the code-T/code flag. Remember to codenice/code it to 20 if you intend to use that mac for something other than a big paperweight if you use all cores at once.  <comment373828> -fc flag should ensure that behaviour. Are you piping or redirecting stdout (the compressed stream)? Show us the command you use. <comment373829> xz -e9vfc  blob  blob.xz . xz is cowardly refusing to spam your terminal with binary gibberish. You have to direct/pipe it somewhere. <comment373989> Omit -fc if you are not redirecting or piping the data. What is -k for? <comment373990> -k, --keep          keep (don't delete) input files <comment373991> Either omit -fc or redirect/pipe data. <comment374026> date "+%Y%m%d_%H%M%S.%s" 20170823_231254.1503547974 <comment374028> Is it possible have less time in this process?, It took 13 hours for finish.... <comment374083> @ChepeQuestn Use -5 instead og -9, and don't use -e. It will be a lot faster.