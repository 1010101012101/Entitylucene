Tool that watches files / folders and upload on change on OS X? <body> I am looking for an SFTP or rsync (?) tool (or script?) that watches a certain folder or certain files and uploads them to a remote server securely on change.  The background:  I've been using Sublime SFTP for that, it works quite well, but sometimes "forgets" its watching duties. Also, I'm giving Atom a go as an alternative editor and there are no tools that live up 100% to Sublime SFTP.  Most editor-based SFTP tools just upload files upon save, but that doesn't cover files built buy gulp or CodeKit.  We've been adding codegulp-sftp/code to our build scripts, but that also has drawbacks.  So what I've been wondering: isn't there a standalone application that simply can watch files or folders for changes and upload them via SFTP, rsync or scp?   <answer228578> Try codecron/code. It's a tool that runs commands at a specific time or interval. It may not be exactly what you're looking for (clone on build) but it works quite well.  To set up cron: from Terminal, type codecrontab -e/code.  This should bring up a text editor (depending on which text editor you set as default) and a nice header explaining what each field does.  precodeminute, hour: does as it says dom: day of month mon: month dow: day of week /code/pre  Now, we'll take a look at your command to do regular backups. A great tool for backups is rsync. You'll need to set up SSH keys for it to work without a password prompt.  Here's the command I use:  precodersync -vzhaPE --delete-after /path/to/local/file ip.address.of.server:/path/to/backup/location /code/pre  Options used: -vzhaPE  precodev: verbose z: compress (good for slow connections. Adjust compression level with --compress-level=&lt;1-9&gt;) h: human readable numbers a: archive mode. This preserves timestamps and practically clones everything over as it were. P: progress bar. E: keep partial files. This means that if the connection was interrupted, rsync will pick up on the partial files and continue where it left off. Good for copying multiple large files over a slow network. delete-after: deletes files from the server that have been deleting from the sending side /code/pre  Options v, h and P are more for human debugging, so we won't introduce them into our cron job.  So, to put it together in cron: (run every ten minutes, for example)  precode*/10 * * * * rsync -zaE --delete-after /path/to/local/file ip.address.of.server:/path/to/backup/location /code/pre  This command will back whatever files in are the directory specified to the server's specified directory emevery ten minutes/em. No duplicates will be made as rsync will delete the files it copies over.  Additionally, if you'd like, you can also run a script that moves the copied backup over to another folder if you're interested in a Time Machine-like versioning functionality. You'll need enough storage space though, and some minor tweaks to the backup command (rsync push to rsync pull). If this is what you're looking for, let me know and I'll expand on it.  <answer229114> I found https://github.com/DmitryKoterov/dklab_realsync/tree/master - the setup is easy (with a config file for each site and a wizard to create it) it's cross-platform, supports password-less login - great!  Just take care when defining what's to be synced or not if your local instance is not 100% the same as the one on the server â€“ realysnc will make them the same!   <answer254895> https://atom.io/packages/remote-sync has a reliable watcher and is less "risky" to use than remote_sync from my other answer. Fine for syncing .css and .js files only.   Doesn't work completely password-less though, I use it with FTPS instead of SFTP  <comment278731> Look at Folder actions - these will kick off a script when a folder change <comment278735> Downvoter, why the downvote? <comment278739> I'm not the downvoter, but I'm looking for a tool that watches constantly for changes, not in intervals. If you build some css or js via gulp, you want it to be moved to your testing environment immediately. Even if it was possible to run the cron every second, it might be too resource intensive, wouldn't it? <comment278740> something like that then? https://sites.google.com/site/andreatagliasacchi/blog/osxautomaticsyncwithfolderactions <comment278748> Ah, I see. I am mistaken in that sense then. Technically, a tool that watches constantly for changes is probably executing a few lines of code every millisecond or so as well, but it's probably more optimised for that sort of purpose. <comment278988> I've got a lead https://trac.cyberduck.io/ticket/2306 <comment278989> `fswatch` with `duck` https://trac.cyberduck.io/wiki/help/en/howto/cli#Watchingchangesindirectorywithfswatchandupload <comment334261> +1'd for thorough answer, even though it was not what was being looked for