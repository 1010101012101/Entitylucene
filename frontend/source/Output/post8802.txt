Free OS X utility to scrape web sites? <body> I want to scrape all the pages and subpages of a web site (by scrape, I mean save all the online content to local HTML files).  What is the best utility to crawl all the pages? Ideally, I would like to specify how many layers deep to scrape.  <answer8803> You have two options:  You can use the codewget/code command-line utility like so:  precodewget -rl 10 /code/pre  Replace the code10/code with the number of levels you would like to recurse down into.  Or, you can use SiteSucker. A GUI application that recursively downloads websites. You can also specify how far down to recurse with SiteSucker.  <answer19807> h3DeepVacuum/h3  GUI for wget, $15.  img src="https://i.imgur.com/2ImVw.png" alt=""  h3cocoawget/h3  (free)  img src="https://i.imgur.com/1PDmA.png" alt=""  <comment9347> SiteSucker is great. <comment21779> Deepvacuum makes me think of Dire Straits: "Money for nothing" :D <comment55559> +1 for SiteSucker. Great tool. Does the job perfectly.