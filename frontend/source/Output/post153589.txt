Trying to get Hadoop to work; "connection refused" in Hadoop and in Telnet <body> I was trying to get Hadoop working in standalone mode on my MBP (OS 10.9.5), but I kept getting "connection refused" errors. I found that codetelnet localhost 9000/code gives the same error, which is what I was told to try as a diagnostic. The same thing happens if I try code127.0.0.1/code instead of codelocalhost/code, or if I try codeftp/code instead of codetelnet/code. However codeping localhost/code and codessh localhost/code work fine.  I had messed around a while ago with setting up an Apache server, and I'm concerned I might have broken something. At some point, I had apparently added the line:  precode127.0.0.1    test.local /code/pre  to code/etc/hosts/code. I also had modified codehttpd.conf/code to use the folder code~/test/code as my codeDocumentRoot/code, and had changed to codeextra/httpd-vhosts.conf/code as well.  I restored the original codehttpd*/code files from the code/etc/apache2/original/code folder, and I restored the code/etc/hosts/code file to its original state. codeapachectl configtest/code gives me:  precodehttpd: Could not reliably determine the server's fully qualified domain name, using &lt;username&gt;.local for ServerName Syntax OK /code/pre  So what do I do? How can I get my computer to stop refusing the connection? I don't know much about networking or servers.  hr  For completeness, here's the original telnet error:  precode$ ssh localhost $ telnet localhost 9000 Trying ::1... telnet: connect to address ::1: Connection refused Trying 127.0.0.1... telnet: connect to address 127.0.0.1: Connection refused Trying fe80::1... telnet: connect to address fe80::1: Connection refused telnet: Unable to connect to remote host /code/pre  and my code/etc/hosts/code file:  precode## # Host Database # # localhost is used to configure the loopback interface # when the system is booting.  Do not change this entry. ## 127.0.0.1   localhost 255.255.255.255 broadcasthost ::1             localhost  fe80::1%lo0 localhost /code/pre  and strongthe Hadoop error which uses the same "connection refused" language as telnet/strong:  precodejava.net.ConnectException: Call From &lt;username&gt;.local/&lt;ip&gt; to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused /code/pre  Following the link in the error (http://wiki.apache.org/hadoop/ConnectionRefused), I read the following:     If the application or cluster is not working, and this message appears in the log, then it is more serious.      ul   liCheck the hostname the client using is correct/li   liCheck the IP address the client is trying to talk to for the hostname is correct./li   liMake sure the destination address in the exception isn't 0.0.0.0 -this means that you haven't actually configured the client with the real address for that   service, and instead it is picking up the server-side property telling it to listen on every port for connections./li   liCheck that there isn't an entry for your hostname mapped to 127.0.0.1 or 127.0.1.1 in /etc/hosts (Ubuntu is notorious for this)/li   liCheck the port the client is trying to talk to using matches that the server is offering a service on./li   liOn the server, try a codetelnet localhost &lt;port&gt;/code to see if the port is open there./li   liOn the client, try a codetelnet &lt;server&gt; &lt;port&gt;/code to see if the port is accessible remotely./li   /ul      None of these are Hadoop problems, they are host, network and firewall configuration issues. As it is your cluster, only you can find out and track down the problem.   and indeed I fail on the second-to-last step, which apparently should work; hence this question.  <answer153592> It's normal getting codeconection refused/code if you codeftp/code or codetelnet/code to localhost, at least if you haven't up and running that kind of servers, and by default, you don't.  The code/etc/hosts/code should look like code127.0.0.1 localhost/code by default. To use apache and avoid the error you mentioned, you must add codeServerName  localhost/code to codehttpd.conf/code. Restart apache and should works fine.  Don't know if this solve Hadoop error too. Can't help you on it. Just try and let me know.  <answer174021> It turns out I had "Remote Login" disabled in System Preferences!  The solution was:     System Preferences - Sharing - Check the "Remote Login" box   Although I also reformatted by hard drive and upgraded to Yosemite, so having a fresh version of Apache probably helped too.  <comment180853> _Should_ I be using Apache? My impression from the Hadoop tutorial (http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation) is that it should work out of the box without any server setup. <comment181125> Would this question be better suited for StackOverflow? I still haven't figured this out.