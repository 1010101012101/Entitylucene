Batch download URLs from a .txt file <body> I have a .txt file with URL's on a separate line.  precodehttp://www.apple.com http://www.google.com http://www.reuters.com /code/pre  I would like to download these webpages as a page source (a .html file) so  I can open them offline in my webbrowser.  I tried to do this with automator, but it doesn't seem to work properly. My Automator workflow consists of 2 steps: "Extract data from text" and "Download URLs". I've looked on the web for already existing solutions, but I haven't found anything I understand.  Can someone create a program with Automator or Applescript (or something else) so I can download these webpages?  The program should work as follows:  ol liThe program reads a .txt file with URLs on a separate line. (The filetype doesn't really matter, as long as it is simple for your program: .csv, .pages, .doc, ...)/li liThe program reads each URL in the file and downloads it as a .html file in order that the webpages are accessible without an internet connection./li liAll the .html files should be saved in a folder, preferably a folder on my desktop with the name "Downloaded html files"/li /ol  Thanks in advance, If there are any questions, don't hesitate to ask. I will respond asap.  <answer214246> strongTo use the following method, you will need to install codewget/code./strong  Make a file with the extension strong.sh/strong in the same directory as your file containing the links and add this text to it:  precodemkdir ~/Desktop/download  while read line; do wget -E -H --directory-prefix=/Users/username/Desktop/download -k -p $line; done &lt; file.txt  cd ~/Desktop/download /code/pre  Make sure to strongedit the script and change username to your username/strong  This reads strongfile.txt/strong for the URLs and runs the wget command multiple times with all the links one-by-one and saves them to a folder named strongdownload/strong on your desktop.  Run it in terminal with code./script.sh/code or whatever you named it. If it shows strongPermission Denied/strong, then run codechmod a+x script.sh/code  <answer214250> The command line is easier (https://superuser.com/a/168625):  Use codewget -i urls.txt/code  An alternate way, using this answer, using Extract URLs from text, then Download URLs. Save it as a service. Select all URLs, right click  Services  Download URLs  <comment258629> `wget` is not part of OS X by default <comment258630> What is a "map" (item 3)? Do you just want the page the URL points to OR the page including all images etc. OR all subpages as well (aka the whole site the URL points to)? <comment258635> @patrix I put the mv command into the shell script, because when I tested it, wget made a directory called -directory-prefix=~ in the directory with my script, which contained Desktop and download. <comment258637> I want a full copy of the webpage. I don't want to save the whole site with all subpages. just the page the URL points to with images etc. Each .html file should be saved in a folder on my desktop. With 'map', i mean folder (translation mistake). Thanks for your response! <comment258640> Yes, because you had both `-P` and `----directory-prefix`, which do the same thing, so you only need one or the other. I've fixed the `wget` options. <comment258646> Could this be done with curl? <comment258653> @amanthethy Yes. Just change the wget command after the do in the loop into curl. I dont know the optio s though <comment258676> Thank you for your contribution! I will repeat the program often so it would be a lot easier if it came in an applescript or automater format instead of terminal commands. <comment258703> `wget` is not installed on OS X. <comment258723> @jBot-42 you can install it easily using homebrew. <comment258727> Concerning your alternate way: I made the [automator program](https://www.dropbox.com/s/uk7kn3p7zjutp4c/URL%20to%20HTML.workflow.zip?dl=0) (saved as a service). While using this program, the URLs are downloaded as html-files and saved in my download folder. Instead, I want the files saved in a new folder on my desktop. How do I automatically create a new folder on my desktop (named "Downloaded html files") where the files are downloaded? Can someone adapt the automator program that I just made so it fits all my needs? Thanks in advance!