How can I limit the amount of memory available to a process? <body> I am developing a program in go; it occasionally ends up allocating very large amounts memory (10G on a machine with 8G of physical memory), causing the system to become unresponsive.  I want to limit the amount of memory the process can allocate.  The usual way I would do this is:  codeulimit -m 4000000 ; ./myprogram/code  ...which should kill my program if it tries to use more than 4GB of memory.  On OS X El Capitan this appears to have no effect; even codeulimit -m 1/code (limiting all programs to just 1kB of memory!) is ineffective.  How can I set an upper bound on the memory available to a particular process?  <answer285508> There are two approaches to constraining your memory usage: Ex post facto, and preemptive. That is to say, you can try to kill your program after it has gotten too large, or you can program it not to get too large in the first place.   If you insist on the ex post facto approach, you can use the following Bash script. This script first finds the amount of memory (as defined by "resident set size") the process with processid pid is using, filters out all non numerical data using grep, and saves the amount as variable n. The script then checks if n is greater than your specified x. If it is, the process with processid pid is killed.  Please note:  ol liYou must replace code&lt;pid&gt;/code with the process id of your program. /li liYou must replace code&lt;x&gt;/code with the rss = "resident set size" (ie real memory size) you don't want the program to exceed.  /li /ol  coden=$(ps -&lt;pid&gt; -o rss | grep '[0-9]') if [ $n -gt &lt;x&gt; ]; then  kill -9 &lt;pid&gt;; fi/code  If you want this to run every y seconds just inclose it in a loop, and tell it to wait for y seconds after every iteration. You could also write a similar command using codetop/code. Your starting point would be codetop -l 1|grep "&lt;pid&gt;"|awk '{print $10}'/code.  @kenorb's answer helped me with my script  hr  While I believe that answers the question, in the long run I believe it is better programming design to take a preemptive approach using manual memory allocation.   Firstly, are you sure the memory usage is really a problem? The Go documentation states:      The Go memory allocator reserves a large region of virtual memory as an arena for allocations. This virtual memory is local to the specific Go process; the reservation does not deprive other processes of memory.   If you still think you have a problem, then I encourage you to manually manage your memory as is done in the C programming language. Since go is written in C, I suspected there would be ways to get into the C memory management/allocations, and indeed there are. See this github repository which,     allows you to do manual memory management via the standard C allocator for your system. It is a thin wrapper on top of malloc, calloc and free from . See man malloc for details on these functions for your system. This library uses cgo.   The use case is given as:     Why would you want this?      When a program is causing memory pressure or the system is running out of memory it can be helpful to manually control memory allocations and deallocations. Go can help you control allocations but it is not possible to explicitly deallocate unneeded data.   This seems like a better long term solution.  If you want to learn more about C (including memory management),  The C Programming language is the standard reference.  <comment349513> When you say _The usual way I would do this_, what do you mean? More specifically, _when_ do you usually do this? And when you do it, do you mean within Mac OS X El Capitan or in another environment? Can you give an example of when you've used it successfully? Basically, I'm just trying to clarify whether this process works for you normally, but just isn't working for this particular program? Or is it not working for you at all but you think it should? <comment349778> By "the usual way I would do this", I mean the way I would do this on other flavours of unix.  I note that I have just discovered that `ulimit -m` no longer works on Linux ( 2.4.30), though `ulimit -v` does still work as expected.  (Like `ulimit -m`, `ulimit -v` also appears to have no effect on OS X.) <comment353412> That sounds like you've got a memory leak in the program you're writing and need to do better garbage collection. Have you read up on memory management in go? <comment353482> No, not a memory leakâ€”just a graph search of a potentially very large state space.  I could add code to abort the search if the various internal structures get too large, but I was expecting to be able to do the same thing (prevent machine from getting wedged by large inputs) with a shell one-liner. <comment360634> I am sure that memory usage is really a problem.  When the program unexpectedly grew to be 2x physical memory size the machine became almost entirely non-responsive due to page thrashing.  It took about an hour between when I hit ^C and when macOS resumed responding to mouse clicks; in the mean time the only evidence that it was not frozen was the quiet noise of the hard drive rapidly ticking about. <comment360635> This is probably a reasonable solution in practice, but unlike ulimit on (non-Darwin) UNIX OSes it depends on being able to allocate enough memory in a timely fashion to successfully execute the kill command.  I'd really rather have process size limits enforced by the kernel. <comment360689> @cpcallen doing some searching it seems the "-m parameter to ulimit has no effect on Linux systems with kernel versions more recent than 2.4.30."  It seems OSX has also picked up on this change. Try the -v option to limit the address space <comment360691> See this answer https://superuser.com/questions/239796/limits-conf-to-set-memory-limits/239797