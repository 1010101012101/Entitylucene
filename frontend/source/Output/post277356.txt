Machine Learning on external GPU with CUDA and late MBP 2016? <body> I would to know what the external GPU (eGPU) options are for macOS in 2017 with the late 2016 MacBook Pro.  I did my research, however on the internet I find a lot of confusing information. Some say it can work, but it requires Windows (dual-boot). Others say, it can only work for the older graphics cards as CUDA is not supported for the newer graphics cards (GTX 1080). Ideally, I would like to run the 1080 GTX of NVIDIA. My only purpose is to use Keras and TensorFlow with it. However, I do not know all the things that are important to get it to work. My question therefore is, is it possible to use TensorFlow with CUDA and eGPU on the late MacBook Pro 2016 (15")? I want to use the graphics card in macOS (with late MacBook Pro 15") as an eGPU (no dual-boot/Windows/Linux partition).  emSide note: I have seen users making use of eGPU's on macbook's before (Razor Core, AKiTiO Node), but never in combination with CUDA and Machine Learning (or the 1080 GTX for that matter). People suggested renting server space instead, or using Windows (better graphics card support) or even building a new PC for the same price that allows you to use a eGPU on Mac. (I do not prefer that option.)/em  <answer277401> eGPU support on macOS is a difficult topic, but I will do my best to answer your question.  Let's begin with graphics cards! For the sake of time, and because we're talking CUDA, we'll stick with Nvidia cards. Any graphics card will work with the proper drivers on Windows. Apple, however, only officially supports a few Nvidia graphics cards, mainly very old ones. However, the Nvidia graphics drivers actually work on almost all of Nvidia's GeForce and Quadro cards, with one big exception. GTX 10xx cards WILL NOT WORK. On any Mac operating system. Period. Nvidia's drivers don't support this card. If you're looking for power, you'll want to look at the GTX 980Ti or Titan X (many good Quadro cards would also work well).  Now that we've got that covered, let's move onto eGPU enclosures. I'm going to assume, because you mentioned specifically eGPUs, that you've budgeted for an actual eGPU enclosure (let's use the AKiTiO Node as an example), instead of a PCIe expansion chassis with an external power supply, as this is not a great idea.  So now we have a graphics card (GTX 980Ti) in an eGPU enclosure (AKiTiO Node) and we want to get it to work. Well, that's easier said than done. I did a bit of eGPU researching towards the end of 2016, and the information I got was relatively confusing, so if anyone has any comments or corrections, please let me know. From what I understand, to utilize the power of the eGPU, you need to plug an external monitor into the eGPU. I don't believe you can run the eGPU without an external monitor in macOS. You will also not see Apple's boot screen on the eGPU-connected monitor (unless you buy a flashed card from MacVidCards), but you should then be able to use the eGPU to drive your graphics.  Assuming you do all of this successfully, you should have a very high powered CUDA-enabled graphics powerhouse.  <answer283903> I was able to get a NVIDIA GTX 1080 Ti working on the Akitio Node on my iMac (late 2013). I'm using a Thunderbolt 2  3 adapter, though on newer Macs you can use the faster TB3 directly.  There are various eGPU set-ups described at eGPU.io, and you might find one that describes your computer/enclosure/card precisely. These tutorials are mostly for accelerating a display with an eGPU, though for training NNs you don't obviously need to follow all the steps.  Here's roughly what I did:  ul liInstall CUDA according to official documentation./li liDisable SIP (Google for a tutorial). It's needed by the eGPU.sh script and later also by TensorFlow./li liRun the automate-eGPU.sh script (with sudo) that everybody at eGPU.io seems to rely on. /li liInstall cuDNN. The files from NVIDIA's website should go under code/usr/local/cuda/code with the rest of your CUDA libraries and includes./li liUninstall CPU-only TensorFlow and install one with GPU support. When installing with codepip install tensorflow-gpu/code, I had no installation errors, but got a segfault when requiring TensorFlow in Python. Turns out there are some environment variables that have to be set (a bit differently than the CUDA installer suggests), which were described in a GitHub issue comment./li liI also tried compiling TensorFlow from source, which didn't work before I set the env vars as described in the previous step./li /ul  From iStat Menus I can verify that my external GPU is indeed used during training. This TensorFlow installation didn't work with Jupyter, though, but hopefully there's a workaround for that.  I haven't used this set-up much so not sure about the performance increase (or bandwidth limitations), but eGPU + TensorFlow/CUDA certainly is possible now, since NVIDIA started releasing proper drivers for macOS.  <comment349162> Thank you for the information. The combination of 980 Ti with an eGPU enclosure seems like a viable option. The only thing is, the Akitio Node (3) seems discontinued and the Razor Core does not ship. Which eGPU enclosure can actually be bought? Akitio Node 2? <comment349167> Well Bizon Box is designed for it, but it's like $500. Let me do some looking... <comment349168> This link might make for some good reading too: http://appleinsider.com/articles/17/01/17/powercolors-thunderbolt-3-devil-box-is-the-easiest-way-to-get-an-external-gpu-on-the-macbook-pro <comment371040> A word of warning: from TensorFlow 1.2 onwards, they are not providing official tensorflow-gpu pip packages. This means we need to build it from sources, which in my experience never works right away. Hopefully there will be 3rd party tutorials on how to compile major releases, but for now I can't for example upgrade to 1.2 or 1.3 if I still want to use my GPU. <comment371175> Managed to compile tensorfow 1.2 from source. Wrote a little tutorial on it: https://medium.com/@mattias.arro/installing-tensorflow-1-2-from-sources-with-gpu-support-on-macos-4f2c5cab8186 <comment373926> CUDA on the 1080 most definitely does work. I was training a network on a 1080 earlier this morning using Keras with TensorFlow backend (on Ubuntu, but still).