How to download a webpage's all images at once? <body> For example, I love military decorations, and here's Wikipedia's Service Ribbon, how can I download all the ribbon images at once instead of clicking them one by one and then select "Save image as"?  <answer121654> h1Automator/h1  Use OS X's strongAutomator.app/strong to find, extract, and save the images from your current web page. The combination of Actions needed are:  ul liGet Current Webpage from Safari/li liGet Contents of Webpages/li liSave Images From Web Content/li /ul  To learn more about using Automator, see Apple's Mac Basics: Automator.  img src="https://i.stack.imgur.com/7bicY.png" alt="Automator"  h2Terminal/h2  An alternative approach is to use codecurl/code through the command line, What's the fastest and easiest way to download all the images from a website.  <answer121733> You can use Firefox and Flashgot, which is an extension which does pretty much exactly what you are looking for.  You can find Flashgot on the official Mozilla addons website here  Flashgot uses a download manager of your choice, either the one integrated in Firefox, curl, wget, or others. Personally I like DownThemAll!.  <answer124530> Using codewget/code:  precodewget http://en.wikipedia.org/wiki/Service_Ribbon -p -A .jpg,.jpeg,.png -H -nd /code/pre  code-p/code (code--page-requisites/code) downloads resources like images and stylesheets even when you don't use code-r/code. code-A/code specifies suffixes or glob-style patterns to accept. code-H/code (code--span-hosts/code) follows links to other domains like codeupload.wikimedia.org/code. code-nd/code (code--no-directories/code) downloads all files to the current directory without creating subdirectories.  You can install codewget/code with codebrew install wget/code after installing Homebrew.  You might also just use codecurl/code:  precodecurl example.tumblr.com | grep -o 'src="[^"]*.jpg"' | cut -d\" -f2 |          while read l; do curl "$l" -o "${l##*/}"; done /code/pre  Downloading images from Tumblr or Blogspot:  precodeapi="http://api.tumblr.com/v2/blog/example.tumblr.com/posts?type=photo&amp;api_key=get from tumblr.com/api" seq 0 20 $(curl -s $api | jq .response.total_posts) |     while read n; do         curl -s "$api&amp;offset=$n" |             jq -r '.response.posts[].photos[].original_size.url'     done | awk '!a[$0]++' | parallel wget -q  curl -L 'http://someblog.blogspot.com/atom.xml?max-results=499' |     grep -io 'href=&amp;quot;http://[^&amp;]*.jpg' |      cut -d\; -f2 |     awk '!a[$0]++' |     parallel wget -q /code/pre  <answer191246> If you use Mac, try "Cliche: Easy Web Image Collector" on the Mac App Store.  According to the app description,  "The easy yet powerful web image collector for the Mac. Cliche enables you to quickly and easily collect web images with powerful handy tools. Just browse the web with Cliche. All web images are already ready for you."  For more information, you can visit the official website: https://machelperprojects.wordpress.com/2015/05/08/cliche-easy-web-image-collector/  <comment141818> It would be much quicked for you to google it. For example http://www.wikihow.com/Download-All-Images-on-a-Web-Page-at-Once <comment141822> @Jaqenhghar That's for windows, does it work on Mac? <comment141936> Hi welcome to AskDifferent, I find if a question is worth answering it is worth upvoting.  Also some instructions and a link may get you more upvotes. Please take the time to read the Answer and Question section of the [help](http://apple.stackexchange.com/help) page. <comment145510> @Deesbek expanded answer :) <comment145666> Where do you get `parallel` from? <comment145689> You can install `parallel` and `jq` with `brew install parallel jq` after installing [Homebrew](http://brew.sh) or with `sudo port install parallel jq` after installing [MacPorts](http://www.macports.org/install.php). <comment227117> Please have a look at the [FAQ], especially the part about self-promotion.