Get text from multiple webpages doesnt work if one URL is down <body> I have creating a simple workflow in Automator which will extract text from a list of URLs and save them into one text file.  The actions are:  ul liGet Specified Text/li liGet Text from Webpage/li liNew Text File/li /ul  Whenever there is a problem with one of the URLs, Automator throws an error and stops working - is it possible to make it ignore the problem URL and continue?   <answer83644> Could you use curl or wget instead?  precodefor u in $(cat urls.txt); do curl -L "$u"; done &gt; output.txt brew install wget wget -i urls.txt -U mozilla -O output.txt /code/pre  <answer83742> The reason is, the "Get Specified Text" does not allow returns (if you are putting more than one line of text).   In order for me to fix this problem (mine was similar), I found an automator text actions pack on line and bought it (really cheap). You can find it here:   http://www.automatedworkflows.com  <comment96324> Im sorry, I have absolutely no idea what you are talking about...even Automator is a struggle for me to understand. Can you advise what I would do with the above code? <comment96406> OP included a terminal command in their answer. Since it's a bit over your head, I'd recommend following Philippe's answer. <comment96413> Thanks I have just bought the text actions pack....but I am having the same problem. I have this setup:The problem is when any supplied URL has a timeout. Is there a way to set a condition where it attempts to retrieve the text, but if unable, moves onto the next URL.