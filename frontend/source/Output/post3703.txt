Ripping a whole website to PDF <body> Adobe Acrobat Pro allows the user to convert an entire website (or subset) to a PDF. Is there anything strongelse/strong available for OS X to do the same thing? I don't want to spend several hundred dollars.  Ideally, it would allow the user to rip a URL and all other URLs on the same "prefix" to a single PDF. For example, given the URL "http://example.com/a_web_page", it would pull "http://example.com/a_web_page/index.html", "http://example.com/a_web_page/a", "http://example.com/a_web_page/b", etc., but not "http://example.com/index.html" to a single PDF.  I have found whole books released under Creative Commons licenses as HTML pages (one per chapter, or section). I would like to capture the entire book to a single PDF to read on my iPad.  Thanks.  <answer3704> It doesn’t do “whole websites” because it would be hard for it to know “how deep” (tho I agree it could be configured in a way of: go down XX levels), but in any case, if your HTML pages contain a full chapter and assuming the books have about 20-30 chapters, is not that “bad”.  With that in mind, I have an inexpensive application that does this job, and it’s called Web Snapper.      strongWhat's so special about Web Snapper?/strong      Web Snapper is drag and drop simple.   When you want to grab a page, you drag   the URL and drop it onto the drop zone   in Web Snapper (or the app icon in the   dock) – it really could't be any   simpler. With the 'Snapper, you can   save pages as scalable/vector PDFs (we   even preserve the links!), or in any   of the image formats that are   supported by MacOS X. If you want to   make a multi-page PDF - simply drag   and drop multiple URLs and then click   the "Save to Multipage PDF".   It may help you because the price tag is $15.  <answer201302> Out company developed a batch process for this at PDFmyURL - with this you can convert an entire website to PDF by using a simple API. If you have the sitemap you can just pass this to the API and otherwise you pass the list of URLs that need to be crawled.  The details are here  <comment3470> Already have it. I tried downloading a complete book using web snapper and gave up after doing several chapters. Acrobat allows you to limit the depth as well as keeping all downloads on a single domain or even limiting to a specific prefix (e.g.: http://domain/directory). <comment3472> @Ralph Yea, it’s not the best tool I guess. You can always use ‘curl’ on the terminal to “leech” the whole website :) (hint: on a Terminal, type: man curl) <comment371852> You could enroll in a monthly subscription to Adobe Acrobat Pro for $25. <comment371853> Instead of buying it outright.