Use curl to download images from website using wildcard? <body> Is there a way to use curl to download non-sequential images with a wildcard? I want to download all the panoramas I upload to my account at 360 Panorama to a local folder so I don't have to do this manually each time.  The images files follow the pattern of: http://occipital.com/images/viewer/XYZ_flat.jpg with the XYZ being random. It also seems from that URL that I need to be logged in or seen as logged in to the site.  <answer60363> So you want to download pictures from http://occipital.com/images/viewer/*_flat.jpg?  This is not possible. You can't treat emviewer/em as a folder. The web is a service that provides you with things when you ask for it. It doesn't have a list of all of the files. Unless you try every file from AAA to ZZZ, this is not possible without a list.  <answer62941> While you can't use a true wildcard you can specify parts within braces ie. curl mysite.{alpha,beta,gamma}.com or number ranges mysite.com/files[0001-0100].txt  See the curl man page for more info  http://curl.haxx.se/docs/manpage.html  So for your files that are random you might do mages/viewer/[000-999]_flat.jpg  You'd end up with lots of file not found but it should get all the files that are there.  <answer66027> You can use this codebash/code code for the actual URL you provided in your comment.  precodefor i in $(for j in {1..8}; do     curl http://occipital.com/user/01ca-255/george-coghill/$j/;   done \   | sed -n 's,.*/viewer/\(......_flat\)_small\.jpg.*,http://occipital.com/images/viewer/\1.jpg,p' ); do   curl -L -o "${i##*/}" "$i"; done /code/pre  Feel free to write this command as one line; all line breaks were only added to increase legibility. You may copy or remove them, whatever you like.  What this code does:  ol liIterate over the 8 pages of your account gallery/li liExtract the image names from the preview images/li liFetch all full-sized images using this list of names/li /ol  If you want to only download files which don't exist, and don't know the number of gallery pages up front, you can adjust the code to this:  precodefor i in $(   j=1;   while curl --max-redirs 0 -L -f \       http://occipital.com/user/01ca-255/george-coghill/$((j++))/; do     :;   done \   | sed -n 's,.*/viewer/\(......_flat\)_small\.jpg.*,http://occipital.com/images/viewer/\1.jpg,p' ); do   [[ -f "${i##*/}" ]] || curl -L -o "${i##*/}" "$i"; done /code/pre  The first code now increments the page number until a redirect or error occurs. The server will redirect you to the last existing page if you try to visit a page after the existing ones. The code[[ -f … ]] ||/code part will execute codecurl/code only if the corresponding file does not exist yet.  <comment69489> I'm not too familiar with curl, as I only just discovered it when looking for a way to accomplish this task and curl (or wget) seemed the solution. <comment69498> See [CURL to download a directory](http://superuser.com/questions/200426/curl-to-download-a-directory) on Super User. <comment69503> Can you add some specific (real) URLs to your question? Maybe we can find a pattern there which would be possible to guess when using `curl`. <comment69599> @patrix here's an actual URL: http://occipital.com/images/viewer/qSJGuD_flat.jpg — seems they don't offer a direct path that's tied to the username. Here's the link to my user account page when I am not logged in: http://occipital.com/user/01ca-255/george-coghill <comment69600> That's what I was thinking was going to be the problem. But let's say there was a URL that this could work on—how would one use curl with non-sequential URLs to automatically check and download new files? Or is it just not possible without a sequential URL pattern? <comment69601> @Arjan I am trying to just download the newer images as they are posted, not the entire directory, and not just one time. Trying to avoid having to go to my account and manually download each image every time I upload new panoramas. <comment69608> No matter what, I'd say it has little to do with Apple and there's many related posts on Super User. (And Jack's answer is correct.) <comment69743> @Arjan As I said, I am not too familiar with all of this but couldn't find anything either way on Stack Exchange about the wildcard aspect. I thought curl was an Apple-only wget variant. It's all very confusing. <comment72626> Not really a Mac OS X question. <comment81831> This did the trick, I have to say it's way over my head but I really appreciate the help! How would I tweak the code to specify the download directory? <comment81951> @GeorgeC: `"${i##*/}"` is the name of the file on your local machine; you may prepend a directory name to that, e.g. `"some/dir/${i##*/}"`. Or you simply `cd` into the desired directory first. <comment83074> Thank you for the additional code.