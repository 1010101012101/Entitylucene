What is the difference between millions and billions in printer/scanner colour settings? <body> img src="https://i.stack.imgur.com/0DGZ5.png" alt="millions/billions"  The "Printers &amp; Scanners" Setting preference pane has two color options: "millions" or "billions" of colors. I can't tell the difference. Is there a reason to choose one over the other? Perhaps specific circumstances?  I'm scanning old photos and determining the tradeoff for time/energy/file size. "Billions" appears to make photos three times the file size of "millions" and I can't discern whether there are archival benefits.  <answer276104> This is color depth.  The Billions setting takes more disk space, so unless you need really high resolution of colors or see banding or just don't care about larger file size or slow scan times, choose Millions.  Your reasoning is perfectly sound. Larger files for no discernible benefit - choose the lower fidelity.  <answer276105> Old photos are unlikely to have colour depth in the millions nevertheless billions. In such a case, there is no point scanning images at anything higher than 24-bit. (See comments below.)  hr  In a nutshell, in certain circumstances some users may opt for billions of colours if they're doing high end photography, graphic design, large format printing, etc and if their hardware supports it.  So, using scanners as an example, colour (or bit depth) is the amount of information a scanner gets from what you’re scanning. Basically, the higher the bit depth, the more colours that can be recognised and the higher your quality of scan will be.  At the risk of oversimplifying things, in summary:  ul liGrayscale scans are 8-bit images recognising 256 levels of gray/li lia 16-bit colour scan can recognise up to 65,536 colours/li lia 24-bit colour scan can recognise up to 16,777,215 colours/li lia 32-bit colour scan can recognise up to 4,294,967,296 colours/li liand so on/li /ul  strongNote:/strong The above is only a basic explanation and doesn't get into alpha channels etc.  strongHardware support/strong  As I mentioned above, opting for billions of colours will also depend on whether your hardware supports it. Obviously, displaying more colors requires more memory. Most computers today will have a GPU with enough memory to support 32-bit colour. Older computers, on the other hand, may only support up to 16-bit colour. Regardless of this, your display also needs to support this too.  Likewise with scanners. Not all scanners can scan at the same bit depth. Using my summary above, a scanner only capable of 24 bit can recognise up to 16,777,215 colours, well short of the embillions/em that some can scan.  strongI can't tell the difference/strong  You stated in your question, em"I can't tell the difference"/em.   This isn't surprising as I doubt many users could tell the difference between 16-bit and 32-bit colour scans, prints or displays. However, users with particular software capable of showing/differentiating between gradients, shadows, transparency, etc that require a wide range of colour combinations may notice a difference, and this is where it gets back to the specific circumstances you alluded to in your question.   strong[EDIT]/strong  IconDaemon's comment prompted me to add an example of when a user may want to scan at a higher resolution and colour depth than what their own computer/display supports.   Last December I had to produce some large-format posters for my sister-in-law and, to do this, I had to use her much older Mac to design it. While her older Mac wasn't capable of displaying billions of colours, her scanner was capable of doing up to 48-bit scans. So I scanned in the required images using 36-bit for colour depth and 600dpi for the resolution.   Then, on her outdated Mac I used Photoshop CS5 to put it altogether and export the files as high-quality press-ready PDFs. These files were then taken to a service bureau which printed the large format posters. However, if the original images were not scanned at a high enough quality, then the large format prints would have been pixelated and the lack of colour information would have resulted in a print where some of the effects (e.g. transparency, etc) would not come across very well (if at all).  <answer276171> Concerning a photograph with poor exposure and thus poor contrast — i.e. mostly almost white or mostly almost black…  If you have a quality scanner (i.e. from AUD300 or so), you can scan this photo in 30-bit colour (at a high resolution, of course), and then use software [which would have come with the expensive scanner] to expand the white-to-offWhite or black-to-offBlack range into white-to-black, and magically show detail that appeared to be just not there.  (I know, because I have done this.)  It is not actually magic, and will not recover the {blackened dark greys} or {whitened light greys}, but it can do enough to genuinely surprise you (provided, again, that the colour detail has not been entirely destroyed).  Note that, in this process of expanding a section of the colour range, the 30 bits of narrow-range detail become 24 bits of full-range detail.  (The technical term here is “gamma”.)  From memory:  the human eye can differentiate 10 [or perhaps it is 4] million colours, under ideal conditions.  (Repeat:  ideal conditions; this includes using a large area of perfectly uniform colour.)  As noted, 24-bit is nearly 17 million.  Going from 16-bit colour (2/3 x 100,000) to 24-bit does make a difference.  Anything more is wasted — in the final output, that is.  (“Monomeeth” includes an example involving a large image printed professionally.  I would not entirely rule out… there being some conditions under which extra colour depth emin the final print/em actually made some material difference — and indeed professionals can use (and then discard) extra colour information as I described above — but it sounds to me as though the issue in that example was the resolution — 600dpi before scaling — rather than the colour depth.)  Note that the above means that a monitor that can display billions of colours (30-bit) is a complete waste (over 24-bit) — regardless of whether it is a cheap marketing ploy or a professional-quality unit (although conversely they seem to make them with either 6-bit colour or 10-bit).  Ditto for a printer.  The principle above (in which one saves an old photo believed lost) applies also to fixing things like colour bias or contrast loss in any photograph; if the final output is 24-bit, then an initial scan at 30 bits is (somewhat) worth doing.  In Apple's photo application, there is a magic-fix button, that does this well, automatically.  In summary:  it is worth scanning in 30-bit, for a perfect final output in 24-bit, but saving a 30-bit scan is just a waste.  <comment347436> As is touched on in the last paragraph, one reason to scan at a higher resolution and colour depth than a machine can present on a typical desktop display is for processing and printing on large-format and/or high-resolution printers. <comment347437> Thanks @IconDaemon - your comment reminded me of an example I could share, so I've edited my answer to include it. :) <comment347448> "Billions of colors" is 30/36-bit color, which requires at most 50% more storage space than 24-bit ("millions") color; certainly nowhere near 1000x, even worst-case. <comment347449> It's probably 30 or 36-bit rather than 32 in this case. But yeah. See also [deep color](https://en.wikipedia.org/wiki/Color_depth#Deep_color_.2830.2F36.2F48-bit.29). <comment347450> 30-bit color (+2 padding bits) = billions = 33% more storage space than 24-bit color. 36-bit color (+4 padding bits) = 67% more storage space, that's your worst case. 1000x space (relative to the millions case) would roughly translate to (2^24)^1000 = [this many colors](http://pastebin.com/6D5D5umY). I don't think there's actually a word for that many, except possibly a "bajillion gazillions". <comment347451> Oh, Apple, I love how your attempts to make things simpler for us backfire and just confuse us and dumb us down. <comment347452> @Monomeeth I'm scanning photos at 600 dpi because the scanner can handle it and I'm trying to approach archival quality, but I'm still not sure how I stand to benefit from using billions instead of millions—is the basic answer that later technology might show it better? Or that I'm wasting three times the space because even with perfect equipment I would still not notice the difference? <comment347454> I flubbed up the math. Thanks all for the check there. feel free to edit in your math/storage knowledge if you think it helps the answer. @JasonC <comment347459> @Wolf In a sense, what you're asking is a matter of opinion. There's really no telling what you _may_ or _may not_ do with those scanned photos in future. And of course, technology changes greatly over time. Scanning photos at 600dpi may be overkill if all you're wanting to do is store them as a way to preserve them for future generations, but who knows - in future people may be viewing images on extremely large walls and therefore a 600dpi photo will look better than a 200dpi photo. However, in commercial printing, 300dpi is often used as the benchmark. <comment347460> @Wolf In terms of millions v billions of colours, one thing I'd keep in mind is the original quality of those photos you're scanning in. It's unlikely that they are going to have that much colour depth to begin with, so picking millions of colours is, in a sense, overkill too. In other words, in your situation there'd be no point scanning images at anything higher than 24-bit. I also remember reading that the human eye can only differentiate something like 10 million colours, in which case 24-bit scanning will actually exceed what the human eye can detect. <comment347481> @Monomeeth, one reason to use the higher color depth is if the distribution of colors in the image doesn't match the computer's distribution.  In particular, 24-bit color isn't very good at distinguishing shades of not-quite-black, while the human eye is.  If you're planning on adjusting the color curves in the scanned images to bring out the shadow detail, you'll want to go higher than 24-bit color. <comment347505> You can not always assign a fixed size gain.  The amount of gain depends on the file format, and compression used.  BMP files are 1:1 and unchangeable, but using TIF you can add LZW compression make size increases guestimates.  JPG don't use all the data so it is not a proportional either.