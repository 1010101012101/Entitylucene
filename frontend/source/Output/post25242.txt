How can I automatically download multiple sites through Coda? <body> I have several sites in Coda that I would like to automatically re-download when I tell it to. The sites are collaboratively managed, and I'd like to download a fresh copy of the sites automatically. I am open to AppleScripting, an Automator workflow, a shell script, a combination of all three, or anything else you suggest. How would I go about doing this?  Edit: Can I write a shell script to download the whole site via FTP, then copy it to my "Sites" folder and overwrite the existing data? I need to be able to do it for more than one site, so I need a way to pass in the domain name, user, password, and the directory I want it to copy to.  <answer26157> You could easily use wget for this and script it any which way you like. Here's a quick example of how you could use it to download and overwrite one of your sites in one line:  precodewget ~/Sites/domain/ ftp://[username]:[password]@ftp.example.com/www/ /code/pre  So to do multiple websites you would use:  precodewget -P ~/Sites/ -i sites.txt /code/pre  And your text file might look something like this:  precodeftp://username:password@ftp.site1.com/www/ ftp://username:password@ftp.site2.com/www/ ftp://username:password@ftp.site3.com/www/ /code/pre  From the wget man page:  precodeRecursive download:   -r,  --recursive          specify recursive download.   -l,  --level=NUMBER       maximum recursion depth (inf or 0 for infinite).        --delete-after       delete files locally after downloading them.   -k,  --convert-links      make links in downloaded HTML or CSS point to                             local files.   -K,  --backup-converted   before converting file X, back up as X.orig.   -m,  --mirror             shortcut for -N -r -l inf --no-remove-listing.   -p,  --page-requisites    get all images, etc. needed to display HTML page.        --strict-comments    turn on strict (SGML) handling of HTML comments.  Recursive accept/reject:   -A,  --accept=LIST               comma-separated list of accepted extensions.   -R,  --reject=LIST               comma-separated list of rejected extensions.   -D,  --domains=LIST              comma-separated list of accepted domains.        --exclude-domains=LIST      comma-separated list of rejected domains.        --follow-ftp                follow FTP links from HTML documents.        --follow-tags=LIST          comma-separated list of followed HTML tags.        --ignore-tags=LIST          comma-separated list of ignored HTML tags.   -H,  --span-hosts                go to foreign hosts when recursive.   -L,  --relative                  follow relative links only.   -I,  --include-directories=LIST  list of allowed directories.   --trust-server-names             use the name specified by the redirection                                    url last component.   -X,  --exclude-directories=LIST  list of excluded directories.   -np, --no-parent                 don't ascend to the parent directory. /code/pre  <comment28501> Would there happen to be a source-control system (CVS, SVN, Git, Mercurial) helping along this "collaboratively managed" website? <comment28521> @VxJasonxV No, it's a custom framework. <comment28566> Good question @VxJasonxV. If you did use SVN, for example, updates would e easy, plus you get the benefit of not having your collaborators randomly overwriting your updates. It's the way to go! <comment28569> @Anthony I really just want to write an Applescript/shell script to do it. I know it can be done, I just don't have the scripting knowledge to know how to do it. <comment29831> Cool! How could I pass in domains, the ~/Sites/domain, users, and passwords? <comment29835> You can create a text file that handles all of them. (updated answer) <comment29846> I don't have time right now to test this, but when I do, I'll let you know. Thanks so much! <comment29850> sure. it's very simple to do what you're wanting; any changes to the syntax would be minor. <comment31782> It's only downloading index.html. How do I download all of the folders and the files in them? <comment31797> put and -r in the arguments to recursively retrieve files; you can also use -l 1 to go down one level for instance. [wget / Recursive Retrieval Options](http://www.gnu.org/software/wget/manual/html_node/Recursive-Retrieval-Options.html) <comment31948> Awesome! I will try it when I get a chance. Thanks so much! <comment31955> No worries; let me know if you have anymore questions - I'm happy to help. <comment33005> I do have one more question (I hope it's only one). How do I download only certain folders/files? It's downloading _everything_ including log files and other invisible files. <comment35043> @daviesgeek, when you said 'Can I write a shell script to download the whole site' I assumed you wanted all of it. Do you want just specific files, or specific folders? You can drill down into more specific directories by specifying them after the `/www/` like `/www/path/to/file/`. Is that what you mean? I've included some `wget` arguments into my original answer that should help.