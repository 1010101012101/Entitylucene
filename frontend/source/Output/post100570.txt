Getting files, all at once, from a web page using curl <body> I would like to download the files, all at once, from the following page using codecurl/code: http://www.ime.usp.br/~coelho/mac0122-2013/ep2/esqueleto/  I tried codecurl http://www.ime.usp.br/~coelho/mac0122-2013/ep2/esqueleto//code and it returned a bunch of lines in the terminal, but did'nt get any files.  <answer100573> Use codewget/code instead. You can install it with codebrew install wget/code or codesudo port install wget/code.  For downloading files from a directory listing, use code-r/code (recursive), code-np/code (don't follow links to parent directories), and code-k/code to make links in downloaded HTML or CSS point to local files (credit @xaccrocheur).  precodewget -r -np -k http://www.ime.usp.br/~coelho/mac0122-2013/ep2/esqueleto/ /code/pre  Other useful options:  ul licode-nd/code (no directories): download all files to the current directory/li licode-e robots.off/code: ignore robots.txt files, don't download robots.txt files/li licode-A png,jpg/code: accept only files with the extensions codepng/code or codejpg/code/li licode-m/code (mirror): code-r --timestamping --level inf --no-remove-listing/code/li licode-nc/code, code--no-clobber/code: Skip download if files exist /li /ul  <answer100576> codecurl/code can only read single web pages files, the bunch of lines you got is actually the directory index (which you also see in your browser if you go to that URL). To use codecurl/code and some Unix tools magic to get the files you could use something like  precodefor file in $(curl -s http://www.ime.usp.br/~coelho/mac0122-2013/ep2/esqueleto/ |                   grep href |                   sed 's/.*href="//' |                   sed 's/".*//' |                   grep '^[a-zA-Z].*'); do     curl -s -O http://www.ime.usp.br/~coelho/mac0122-2013/ep2/esqueleto/$file done /code/pre  which will get all the files into the current directory.   For more elaborated needs (including getting a bunch of files from a site with folders/directories), codewget/code (as proposed in another answer already) is the better option.  <answer121112> For those of us who would rather use an application with a GUI, there is the inexpensive shareware program DeepVacuum for Mac OS X, which implements codewget/code in a user-friendly manner, with a list of presets that can handle commonly-needed tasks. You can also save your own custom configurations as presets.  img src="https://i.stack.imgur.com/bjtWv.jpg" alt="enter image description here"  <answer243136> Ref: http://blog.incognitech.in/download-files-from-apache-server-listing-directory/  You can use following command:  precodewget --execute="robots = off" --mirror --convert-links --no-parent --wait=5 &lt;website-url&gt; /code/pre  h3Explanation with each options/h3  ul licodewget/code: Simple Command to make CURL request and download remote files to our local machine./li licode--execute="robots = off"/code: This will ignore robots.txt file while crawling through pages. It is helpful if you're not getting all of the files./li licode--mirror/code: This option will basically mirror the directory structure for the given URL. It's a shortcut for code-N -r -l inf --no-remove-listing/code which means:  ul licode-N/code: don't re-retrieve files unless newer than local/li licode-r/code: specify recursive download/li licode-l inf/code: maximum recursion depth (inf or 0 for infinite)/li licode--no-remove-listing/code: don't remove '.listing' files/li /ul/li licode--convert-links/code: make links in downloaded HTML or CSS point to local files/li licode--no-parent/code: don't ascend to the parent directory/li licode--wait=5/code: wait 5 seconds between retrievals. So that we don't thrash the server./li licode&lt;website-url&gt;/code: This is the website url from where to download the files./li /ul  Happy Downloading :smiley:  <answer287190> You can use httrack available for Windows/MacOS and installable via Homebrew.  <comment188577> `wget -r -np -k http://your.website.com/specific/directory`. The trick is to use `-k` to convert the links (images, etc.) for local viewing. <comment255951> Thank you. This is a nice solution and providing working example is great ! <comment273243> `brew` and `port` doesn't work for me to install wget. What should I do? <comment301520> @HoseynHeydari : you can use rudix.org for compiled binaries for osx. so you need to install rudix and then use : sudo rudix install wget <comment340269> The option `-k` does not always work. E.g., if you have two links pointing to the same file on the webpage you are trying to capture recursively, `wget` only seems to convert link of the first instance but not the second one.